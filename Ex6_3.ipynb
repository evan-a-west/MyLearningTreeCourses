{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Exercise 6.3:\n",
    "# Working with Naïve Bayes in Python\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "#### In this exercise, you will work with a naive bayes model on unstructured data in Python. This exercise allows you to predict a target variable from a number of predictor variables. The goal is to show you how a naive bayes model can be used to predict unknown values from a model trained on an existing data set.\n",
    "\n",
    "### Overview\n",
    "\n",
    "You will work on a data set called sms_spam that you will import from a csv file. You will:<br>\n",
    "● Preprocess the unstructured data into a format suitable for naive bayes<br>\n",
    "● Examine the predictor variables<br>\n",
    "● Train a naive bayes model that can be used to make future predictions<br><br>\n",
    "\n",
    "1. ❏ Import the **CountVectorizer** library from **sklearn.feature_extraction.text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ❏ Import the **MultinomialNB** library from **sklearn.naive_bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ❏ Import the **pandas** library and use the **read_csv()** function to import the **sms_spam.csv** dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5574, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['type', 'text'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"sms_spam.csv\")\n",
    "print(data.shape)\n",
    "data.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ❏ Count the unique values in the target variable **type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['ham' 'spam']\n"
     ]
    }
   ],
   "source": [
    "print(data.type.nunique())\n",
    "print(data.type.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ❏ Examine the proportions in the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     5574\n",
       "unique       2\n",
       "top        ham\n",
       "freq      4827\n",
       "Name: type, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.type.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n",
      "4827\n",
      "747\n",
      "0.8659849300322928\n",
      "0.1340150699677072\n"
     ]
    }
   ],
   "source": [
    "totalCount = data.type.shape[0]\n",
    "hamCount = data[data['type'] == 'ham'].shape[0]\n",
    "spamCount = data[data['type'] == 'spam'].shape[0]\n",
    "print(totalCount)\n",
    "print(hamCount)\n",
    "print(spamCount)\n",
    "\n",
    "propHam = hamCount/totalCount\n",
    "propSpam = spamCount/totalCount\n",
    "print(propHam)\n",
    "print(propSpam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865985\n",
       "spam    0.134015\n",
       "Name: type, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Faster way!\n",
    "data['type'].value_counts() / len(data['type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. ❏ Separate the **text** variable into a dataframe called **Pred**, and the **type** variable into a dataframe called **target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred = data['text']\n",
    "target = data['type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. ❏ Split the dataset into training and test datasets using the **train_test_split()** function<br><br>\n",
    "*Hint: train_test_split will need to be imported from the sklearn.model_selection library*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5016,)\n",
      "(558,)\n",
      "(5016,)\n",
      "(558,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Pred, target, test_size=0.1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. ❏ Use the **CountVectorizer()** and **fit_transform()** functions to produce a Term Document Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5016, 8205)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "X_train_termDocMat = cv.fit_transform(X_train)\n",
    "X_train_termDocMat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. ❏ Assign the target values from the training dataset into a variable called **targets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_train = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. ❏ Train the Naive Bayes model with the **MultinomialNB()** function using the Term Document Matrix and the **targets** variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.0, 'class_prior': None, 'fit_prior': True}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X=X_train_termDocMat, y=targets_train)\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. ❏ Vectorize the predictor data in the test dataset into token counts<br><br>\n",
    "*Hint: Use the initialized CountVectorizer() function from step 8*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5016, 8205)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_termDocMat = cv.transform(X_test)\n",
    "X_train_termDocMat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. ❏ Use the trained model to make predictions for the vectorized test dataset, and display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham',\n",
       "       'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'spam', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham',\n",
       "       'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham',\n",
       "       'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'spam', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham',\n",
       "       'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham',\n",
       "       'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham',\n",
       "       'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam',\n",
       "       'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'spam', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham',\n",
       "       'spam', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham',\n",
       "       'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham',\n",
       "       'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam'],\n",
       "      dtype='<U4')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test_termDocMat)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. ❏ Evaluate the model using the **classification_report()** function<br><br>\n",
    "*Hint: This will need to be imported from sklearn.metrics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      0.99       491\n",
      "        spam       0.97      0.93      0.95        67\n",
      "\n",
      "    accuracy                           0.99       558\n",
      "   macro avg       0.98      0.96      0.97       558\n",
      "weighted avg       0.99      0.99      0.99       558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. ❏ Evaluate the model using the **confusion_matrix()** function<br><br>\n",
    "*Hint: This will need to be imported from sklearn.metrics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[489,   2],\n",
       "       [  5,  62]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>**Congratulations! You have completed the exercise.**</center>"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAC8AAAAgCAYAAACCcSF5AAAFlElEQVRYCb2YW0hUXRTHdWJKMtIsyuyCqVFQRIVhN0UriKgwKnqSekgoIqOIHkpfii5vUmQ9dYEeg3wKwgiCQf38TGecGa9pauOkjZmN19SZ8f/xX7XPN5nHzozmgTP7nLX3Xvu31l577X0mAtO4/L29aD58GJUREagyeL9jO7MZ7vx8jAcC0xgdiAi390hzM2q3bsW/BqEnGlcREYGWEycwPjoaLkJ48ENVVXCsWRM2uDKEBjQfPYrxkZGwDAjZ8/1v3qBm6dJpg/9iQHY2AsPDIRsQEnzv8+eojo6eMfBfDDh0CH6vNyQDDMN3P3yIdyZTSItTwRkpGUJNmZnwf/1q2ABD8J23bom3JVOEuUCNGtCYkQFfT48hA/4I77p4EfTK3wZXxnGsxvR0+Lq7/2iALry/vx9tJ0/OKrgygOm3YedOjHV1TWmALnzPo0con0WPK3BVyhrYswfjPp+uAbrwnnv3NK/TE1TGO3hT4s6q5BPL4F1Xrz9Bg+uUDsrY3xYXh8DgYOjw3Q8eiOc5wIdjx+C6cAGu8+fRuGuXKKbyug0bfsjz8vDx7Fl8PHMGrrw8kdWnpgoYQRjD7Esd7w8c0PpzHTVlZaGd/X7Wfzx3Dg3bt4tT7MuXIzA0FDq85+5dVJrN+FZc/Ftnd0EByk0mtJ8+/VudEnRcuYIKkwlfnj5VIq30lpTAGhsrgJPpH/f70XL8OGwLFoQH33nzpuRdjtjz5MkPT8THY7CyEsN2O2yxsaiaOxfWmBjYExIw0tqKMY8HzsREkdHwT9euCXDf69ewJyejOiYGXbdvi+zbixcyMz3Pnsl7Y2YmKmNi0JaTI+8DpaWwr1gBJg69Szfmu+7cQf3mzdJvzO3Gp+vX4UxJ0aa8KjJS0ifDp9psBg9qzA70FmNXZB8+yLnFmZQkMrblPex0il464Mvjx/LsSEqChVkmNVXe2caRmAh/X5+8T/ajC++5f19i3n31KvwDA1rfAYsFjVlZAsH1wLi1RkdjpKUFY58/oyYuTjzK2WC8jrx/D2Uo2xPe++qV6GtIS4OnqEieh6xW9Fss2gLtuHwZtoULtXcNIOhBF54L9p+f2cS+ciVasrPRW1wsyniMrdu0SUD04GsWLxav+To7YY2K0owlPEOCV+26deCxg9eQzSbwfSUlaM/NFQfULFsWXswzNttyczFcW4vG3btlSjmtnwoKZDBmF2aSyeAJSDljnRdhuGfwZiYZHxuDz+NB1Zw52oJ2rl8v9dSpbsZ8WNlGYj4tTQbnWaMjPx9cxIH+fhm8bsuWXzw/2tEBn9cLepzgkiIzMrSj7ufCQnTeuIHAzxBszcmRmeVJlVf9tm3Sh6GlwkuFnjSY5Ec/bIqKUMaPhYMHZYExVPjRwClv2r9fCwPx/Pz5GCgrk6mviY0VeALQgIb0dHhfvkTg+3fpP1hRIR8grOMMMeaZEBiGlM0YPLOGKDSZJG4ZuwpKDaLK6nnzUM36yEgNQLUVA6OiRAdl1Kv1M5t/yE0mTcY6Gha25z2FhTKtVMTBg281cHCp6oNl6lnVqVLJWU4mo5zw/GIL63jAzYi7YPBUBg/6t585O45VqxCY4vtWN+a5PgbevoV10aJZN4AO47hMm1NdU8KLARYLbEuWaAt0Njxek5CAwfLyqbil7o/wbDVYWgrbDP5joOcAepxnIJ6djFyG4MWAsjLY4uP/WggR3Llxo5yRjICzjWF4Nub5wz4DfzZN9DwXZ/2OHRh1u41yS7uQ4Nlj2OEAT4AzlYWop2nfPvhC+MtDWRgyPDt+r62FIzl5WgYwv9PjLUeOaEcGBWW0DAteDGhogHPtWgFQG83EcmJ4BL8TvPXUqdn/o1V5ZtTlQtPevXCsXg1ncvL/d0oK+AHCI0MwsHrm7um6dEmpCbv8D7qC8IEdzCaUAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>**This is the end of the exercise.**</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
